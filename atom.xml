<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://chensi01.github.io/</id>
    <title>Gridea</title>
    <updated>2021-03-26T00:30:21.225Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://chensi01.github.io/"/>
    <link rel="self" href="https://chensi01.github.io/atom.xml"/>
    <subtitle>温故而知新</subtitle>
    <logo>https://chensi01.github.io/images/avatar.png</logo>
    <icon>https://chensi01.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, Gridea</rights>
    <entry>
        <title type="html"><![CDATA[2.感知机]]></title>
        <id>https://chensi01.github.io/post/2gan-zhi-ji/</id>
        <link href="https://chensi01.github.io/post/2gan-zhi-ji/">
        </link>
        <updated>2021-03-25T06:25:02.000Z</updated>
        <content type="html"><![CDATA[<h2 id="1本章重点">1.本章重点</h2>
<ul>
<li>感知机的模型、策略和算法。</li>
<li>感知机学习算法的收敛性和对偶形式。</li>
</ul>
<h2 id="2习题">2.习题</h2>
<h3 id="习题21">习题2.1</h3>
<p><strong>感知机模型是线性分类模型</strong>，对应于特征空间中将实例划分为正负两例的分离超平面。由下面的XOR的示意图可得，不存在一条直线将正类和负类分隔开，即<strong>XOR问题是线性不可分的</strong>。因此感知机不能表示异或。<br>
<img src="https://chensi01.github.io//post-images/1616659407689.png" alt="" loading="lazy"></p>
<pre><code class="language-python">#根据输入空间X和XOR规则生成实例
import pandas as pd
X,data=[-1,1],[]
for x1 in X:
    for x2 in X:
        y = -1 if (x1==x2) else 1
        data.append([x1,x2,y])
data = pd.DataFrame(data,index=None,columns=['x1','x2','y'])
print(data)
</code></pre>
<pre><code>   x1  x2  y
0  -1  -1 -1
1  -1   1  1
2   1  -1  1
3   1   1 -1
</code></pre>
<pre><code class="language-python">#根据XOR的数据实例绘制示意图
import matplotlib
from matplotlib import pyplot as plt 
plt.xlim(-2,2)
plt.ylim(-2,2)
plt.xticks(range(-2,3))
plt.yticks(range(-2,3))
plt.xlabel('x1')
plt.ylabel('x2')
plt.plot(data[data['y']==1]['x1'],data[data['y']==1]['x2'],'ro')
plt.plot(data[data['y']==-1]['x1'],data[data['y']==-1]['x2'],'gx')
plt.show()
</code></pre>
<h3 id="习题22">习题2.2</h3>
<pre><code class="language-python">import numpy as np
import pandas as pd
X_train = np.array([[3,3],[4,3],[1,1]])
Y = np.array([1,1,-1])
</code></pre>
<pre><code class="language-python">import matplotlib.pyplot as plt
def plot(w,b):
    # 绘制感知机
    X1 = [0,5]
    X2 = [-(b+w[0]*x1)/(w[1]+1e-7) for x1 in X1]
    plt.plot(X1,X2)
    #     
    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.xlim(0,5)
    plt.ylim(-3,5)
    plt.xticks(range(6))
    plt.yticks(range(-3,6))
    # 绘制训练数据
    plt.plot(X_train[Y==1][:,0],X_train[Y==1][:,1],'ro')
    plt.plot(X_train[Y==-1][:,0],X_train[Y==-1][:,1],'go')
    plt.show()
</code></pre>
<pre><code class="language-python">class Perceptron:
    def __init__(self):
        self.max_iter = 100
        self.lr = 1
        self.input_dim = 2
        self.build_model()
    def build_model(self):
        self.w = np.zeros(self.input_dim)
        self.b = 0
    def predict(self,x):
        output = np.matmul(self.w,x)+self.b
        return np.sign(output)
    def fit(self,X_train,Y):
        cur_iter = 0
        while cur_iter&lt;self.max_iter:
            fail_count = 0
            for x,y in zip(X_train,Y):
                y_hat=self.predict(x)
                if y*(np.matmul(self.w,x)+self.b)&lt;=0:
                    fail_count+=1
                    self.w += self.lr*y*x
                    self.b += self.lr*y
                    plot(self.w,self.b)
            if fail_count==0:
                break 
</code></pre>
<pre><code class="language-python">model = Perceptron()
model.fit(X_train,Y)
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://chensi01.github.io//post-images/1616670259364.png" alt="" loading="lazy"></figure>
<h3 id="习题23">习题2.3</h3>
<ul>
<li>凸壳可以看作是点集合的边界，可以用集合内所有点的线性组合构造。在二维的欧氏空间中，凸壳可以想做一条刚好包围这所有点的橡皮圈。<br>
https://datawhalechina.github.io/statistical-learning-method-solutions-manual/#/chapter2/chapter2?id=%e4%b9%a0%e9%a2%9822</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[1.统计学习方法概论]]></title>
        <id>https://chensi01.github.io/post/1tong-ji-xue-xi-fang-fa-gai-lun/</id>
        <link href="https://chensi01.github.io/post/1tong-ji-xue-xi-fang-fa-gai-lun/">
        </link>
        <updated>2021-03-24T02:25:19.000Z</updated>
        <content type="html"><![CDATA[<h1 id="1本章内容">1.本章内容</h1>
<h2 id="基本概念">基本概念</h2>
<ul>
<li>统计学习是关于计算机基于数据构建概率统计模型，并运用模型对数据进行预测与分析的学科。</li>
<li>统计学习关于数据的基本假设：同类数据具有一定的统计规律性。</li>
<li>监督学习关于数据的基本假设：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span></span></span></span>具有联合概率分布<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo separator="true">,</mo><mi>Y</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(X,Y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span></span></span></span>，且训练数据和测试数据是依<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo separator="true">,</mo><mi>Y</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(X,Y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span></span></span></span>独立同分布产生的。</li>
<li>监督学习关于模型的假设：假设模型属于某个假设空间。</li>
<li>在监督学习中，输入和输出对称为样本；在无监督学习中输入是样本。</li>
</ul>
<h2 id="统计学习三要素模型-策略和算法">统计学习三要素：模型、策略和算法</h2>
<ul>
<li>损失函数是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">f(X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span></span></span></span>的非负实值函数。</li>
<li>损失函数度量模型一次预测的好坏，风险函数（期望损失）度量平均意义下模型预测的好坏。</li>
<li>期望风险：指损失函数的期望，代表理论上模型关于联合概率分布<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo separator="true">,</mo><mi>Y</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(X,Y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span></span></span></span>的平均意义下的损失。学习的目标是选择期望风险最小的模型，需要用到联合分布，但联合分布未知，所以监督学习是一个病态问题。</li>
<li>经验风险：模型关于训练数据集的平均损失。</li>
<li>结构风险：在经验风险上加上表示模型复杂度的正则化项。</li>
</ul>
<h2 id="正则化与交叉验证">正则化与交叉验证</h2>
<ul>
<li>从贝叶斯的角度看，正则化项对应模型的先验概率。可以假设复杂的模型有较小的先验概率，简单的模型有较大的先验概率。</li>
</ul>
<h2 id="泛化能力">泛化能力</h2>
<ul>
<li>模型对未知数据预测的误差即为泛化误差，即模型的期望风险。</li>
<li>泛化误差上界：通过比较学习方法的泛化误差上界来比较优劣。</li>
<li>泛化误差上界的性质：（1）样本容量的函数；（2）假设空间大小的函数。</li>
</ul>
<h2 id="生成模型与判别模型">生成模型与判别模型</h2>
<ul>
<li>监督学习方法分为生成方法和判别方法。（1）生成方法学习联合概率分布<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo separator="true">,</mo><mi>Y</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(X,Y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span></span></span></span>，然后求出条件概率分布<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(Y|X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span>；（2）判别方法直接学习条件概率分布<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(Y|X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span>或决策函数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">f(X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span>。</li>
</ul>
<h1 id="2习题">2.习题</h1>
<ul>
<li>极大似然估计（Maximum Likelihood Estimation，MLE）和贝叶斯估计（Bayesian Estimation）是统计推断中两种最常用的参数估计方法</li>
<li><a href="https://datawhalechina.github.io/statistical-learning-method-solutions-manual/#/chapter1/chapter1">习题解答</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/61593112">极大似然估计和贝叶斯估计</a></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Hello Gridea]]></title>
        <id>https://chensi01.github.io/post/hello-gridea/</id>
        <link href="https://chensi01.github.io/post/hello-gridea/">
        </link>
        <updated>2018-12-11T16:00:00.000Z</updated>
        <summary type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
]]></summary>
        <content type="html"><![CDATA[<p>👏  欢迎使用 <strong>Gridea</strong> ！<br>
✍️  <strong>Gridea</strong> 一个静态博客写作客户端。你可以用它来记录你的生活、心情、知识、笔记、创意... ...</p>
<!-- more -->
<p><a href="https://github.com/getgridea/gridea">Github</a><br>
<a href="https://gridea.dev/">Gridea 主页</a><br>
<a href="http://fehey.com/">示例网站</a></p>
<h2 id="特性">特性👇</h2>
<p>📝  你可以使用最酷的 <strong>Markdown</strong> 语法，进行快速创作</p>
<p>🌉  你可以给文章配上精美的封面图和在文章任意位置插入图片</p>
<p>🏷️  你可以对文章进行标签分组</p>
<p>📋  你可以自定义菜单，甚至可以创建外部链接菜单</p>
<p>💻  你可以在 <strong>Windows</strong>，<strong>MacOS</strong> 或 <strong>Linux</strong> 设备上使用此客户端</p>
<p>🌎  你可以使用 <strong>𝖦𝗂𝗍𝗁𝗎𝖻 𝖯𝖺𝗀𝖾𝗌</strong> 或 <strong>Coding Pages</strong> 向世界展示，未来将支持更多平台</p>
<p>💬  你可以进行简单的配置，接入 <a href="https://github.com/gitalk/gitalk">Gitalk</a> 或 <a href="https://github.com/SukkaW/DisqusJS">DisqusJS</a> 评论系统</p>
<p>🇬🇧  你可以使用<strong>中文简体</strong>或<strong>英语</strong></p>
<p>🌁  你可以任意使用应用内默认主题或任意第三方主题，强大的主题自定义能力</p>
<p>🖥  你可以自定义源文件夹，利用 OneDrive、百度网盘、iCloud、Dropbox 等进行多设备同步</p>
<p>🌱 当然 <strong>Gridea</strong> 还很年轻，有很多不足，但请相信，它会不停向前 🏃</p>
<p>未来，它一定会成为你离不开的伙伴</p>
<p>尽情发挥你的才华吧！</p>
<p>😘 Enjoy~</p>
]]></content>
    </entry>
</feed>