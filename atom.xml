<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://chensi01.github.io/</id>
    <title>Chensi&apos;s Blog</title>
    <updated>2021-03-29T03:34:23.873Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://chensi01.github.io/"/>
    <link rel="self" href="https://chensi01.github.io/atom.xml"/>
    <subtitle>Welcome</subtitle>
    <logo>https://chensi01.github.io/images/avatar.png</logo>
    <icon>https://chensi01.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, Chensi&apos;s Blog</rights>
    <entry>
        <title type="html"><![CDATA[2021 - 影视听FLAG]]></title>
        <id>https://chensi01.github.io/post/2021-ying-shi-ting-flag/</id>
        <link href="https://chensi01.github.io/post/2021-ying-shi-ting-flag/">
        </link>
        <updated>2021-03-28T11:48:38.000Z</updated>
        <content type="html"><![CDATA[<p>BOOKS</p>
<ul>
<li>少有人走的路</li>
<li>笑到最后</li>
<li>爱的艺术️️✔️</li>
<li>亲密关系</li>
<li>浪潮之巅</li>
<li>深度学习推荐系统✔️</li>
</ul>
<p>MOVIES &amp; SHOWS</p>
<ul>
<li>瘦身十律</li>
<li>本杰明巴顿奇事✔️</li>
<li>遗愿清单</li>
<li>活着</li>
<li>教父</li>
<li>年轻气盛</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[《爱的艺术》]]></title>
        <id>https://chensi01.github.io/post/lesslessai-de-yi-zhu-greatergreater/</id>
        <link href="https://chensi01.github.io/post/lesslessai-de-yi-zhu-greatergreater/">
        </link>
        <updated>2021-03-28T10:53:58.000Z</updated>
        <summary type="html"><![CDATA[<p>爱，并不是任何人都可以享受的生活情趣。爱是一门艺术。</p>
]]></summary>
        <content type="html"><![CDATA[<p>爱，并不是任何人都可以享受的生活情趣。爱是一门艺术。</p>
<!-- more -->
<h1 id="序">序</h1>
<ul>
<li>爱，并不是任何人都可以享受的生活情趣，而是与人的成熟程度相关联的。如果一个人不是及其主动地发展自己的全部人格，使自己具有一种能动的生活态度，那么对爱付出的努力必将如同付诸流水。</li>
</ul>
<h1 id="第一章-爱是一门艺术吗">第一章 爱是一门艺术吗？</h1>
<hr>
<ul>
<li><font color=MediumTurquoise>爱是一门艺术</font>，正如生活是一门艺术；想学会如何爱，就必须想学习音乐、绘画等艺术一样，需要知识和努力。</li>
<li>学习艺术的过程分为<font color=MediumTurquoise>精通理论和善于实践</font>。除此之外，要成为任何一门艺术的大师，还必须认为这门艺术是<font color=MediumTurquoise>最高的旨趣</font>所在。而尽管人们对爱的渴望根深蒂固，但我们总是把成功、威望、金钱、权力看得比爱重要并投入大量精力。</li>
</ul>
<hr>
<p><strong>大多数人觉得爱不需要学习的原因：</strong></p>
<ol>
<li>
<p>把爱的问题看作<font color=MediumTurquoise>“被爱”</font>的问题，而不是主动去爱和爱的能力的问题。</p>
<p>人们研究“如何能够被爱，如何变得可爱”。变得可爱的方法和变得成功是一样的，那就是“赢得朋友，影响他人”。</p>
</li>
<li>
<p>把爱的问题看作关于<font color=MediumTurquoise>“对象”</font>的问题，而不是身心能力的问题。</p>
<p>与该问题紧密练习的是当代文化的一个典型特征：整个文化以购买欲和互利交换的观念为基础。爱情观念也遵循同样的交换模式。只有当对方“人性商品”和自己可能交换时，坠入情网的感觉才会发展起来。我尽力满足成交条件。两个人考虑各自的交换价值的限度，认为对方是市场上能得到的最佳对象，再发展恋爱。</p>
</li>
<li>
<p>把“堕入”爱网时的<font color=MediumTurquoise>最初体验</font>和存在于爱之中的持久状态混淆。</p>
<p>fall in love v.s. be in love。对于处于孤立状态的人来说，从素不相识到变得亲近是妙不可言的，但这类爱好景不长。两个人越是熟悉，对立、失望和厌倦就会扼杀兴奋。强烈的迷恋可能只证明了先前的孤独程度。</p>
</li>
</ol>
<hr>
<h1 id="第二章-爱的理论">第二章 爱的理论</h1>
<h2 id="21-爱-对人类存在问题的解答">2.1 爱-对人类存在问题的解答</h2>
<h3 id="爱可以克服分离实现融合">爱可以克服分离，实现融合</h3>
<hr>
<ul>
<li>人生而面临不确定的状态：唯有过去是确定的，关于未来则只有死亡是确定的。</li>
<li>人意识到自己是一个分离的实体，意识到生命短促、生不由己、无依无靠。这种分离的体验是严重焦虑的根源。</li>
<li>人类最深切的需要，就是克服分离，实现结合。</li>
</ul>
<hr>
<p>常见的“克服分离”的途径只是对“存在问题”的片面回答，全面的答案在于实现全面的人与人之间的融合，在于爱。爱是对人类存在问题的回答。</p>
<ol>
<li>
<p><font color=MediumTurquoise>狂欢状况</font>是短暂的联合。</p>
<ul>
<li>如性高潮、酗酒、毒瘾。</li>
<li>狂欢的体验结束之后，会感到更加分离，于是被迫地更加求助于狂欢。结果只能是越加强烈的分离感。</li>
</ul>
</li>
<li>
<p><font color=MediumTurquoise>与群体融合</font>是虚假的联合。</p>
<ul>
<li>群体融合下，个人的自我消失，归属于群体的自我，变成一样的原子人。</li>
<li>不同的制度或是采取威胁恐吓的方式，或是启发和宣传的手段，导致个人遵从集体的习惯甚至观点。然而每个人都确信自己在遵从自己的愿望。</li>
<li>这种融合平静、关注精神而非肉体，因而不足以抚慰分离焦虑。其唯一好处在于其永久性：兹有被引入这种相一致的模式后，人直到葬礼都恪守这一模式。</li>
</ul>
</li>
<li>
<p><font color=MediumTurquoise>创造性活动</font>不是人与人之间的联合。</p>
<ul>
<li>创造者将材料看作外部世界，将自身和材料融于一体。</li>
<li>创造性活动是必须由“我”计划、生产、看到结果。而现代社会中，工作者作为原子机器，不再产生这种融合。</li>
</ul>
</li>
</ol>
<hr>
<h3 id="共生性融合-vs-成熟的爱">共生性融合 v.s. 成熟的爱</h3>
<ul>
<li>并不是人与人之间的融合都是“爱”。</li>
<li>爱是在保存人的完整性、人的个性条件下的融合。爱是人的一种主动能力。</li>
</ul>
<hr>
<p>“共生性融合”</p>
<p>什么是共生性融合？</p>
<ul>
<li>生物学模式：孕妇与胎儿，互为需要。</li>
<li>心理共生：肉体上独立，心理上相互依附。</li>
</ul>
<p>共生性融合的形式：受虐狂和施虐狂。</p>
<ol>
<li>
<p>受虐狂</p>
<ul>
<li>共生性融合的消极形式。屈从。</li>
<li>通过成为另一人的重要部分来逃避分离感：他是一切，我只是他的一部分。作为一部分，我是崇高、权力以及确定的一部分。偶像崇拜机制。</li>
<li>屈从于另一个人，或命运、疾病、有节奏的音乐、毒品带来的狂欢状态。不必做决定，不必冒任何风险。人抛弃了自我的完整性。</li>
</ul>
</li>
<li>
<p>施虐狂</p>
<ul>
<li>共生性融合的积极形式。统治。</li>
<li>通过别人的崇拜来抬高和强化自己，用他人的屈从使自己的力量膨胀。</li>
</ul>
</li>
</ol>
<p>评价：</p>
<ul>
<li>积极和消极形式的共生性融合都是不完整的联合。二者相互依赖，无法独自生存。</li>
<li>受虐和施虐的态度往往在一个人身上用于对待不同对象。希特勒以施虐狂的方式对待人民，以受虐狂的方式对待命运、历史和自然的更高力量。</li>
</ul>
<hr>
<h3 id="爱是主动给予">爱是主动给予</h3>
<hr>
<ul>
<li>活动的主动性：如果人被不安全感和孤独感、或野心和贪婪驱使而工作，那就不是主动的工作。</li>
<li>爱是一种主动的活动，而不是被动的情感。爱主要是给予，而不是接受。</li>
</ul>
<hr>
<p>非生产型取向的人认为给予是剥削和牺牲：</p>
<ul>
<li>市场性格：给予是为了换取接受，否则就是被欺骗。</li>
<li>牺牲取向：给予是牺牲，是德行。人应该给予，但给予是痛苦的。</li>
</ul>
<p>对于生产型性格，给予是生命的表达：</p>
<ul>
<li>在给予中体验我的能力，表示我生命的存在，因此给予本身是快乐的。</li>
<li>最基本的例子在性领域：男性如果有性交能力，就禁不住要给予；如果不能给予，就不具有性交能力。</li>
<li>在物质上，给予意味着富有。人，因给予而不是拥有而富有。</li>
<li>积存取向：这类人因损失而焦虑，是贫穷的表现。</li>
</ul>
<hr>
<p>最重要的给予并非物质而是生命：</p>
<ul>
<li>给予生命的表达方式和证明方式：生命的活力，个人的快乐、旨趣、理解、知识、幽默、悲哀。</li>
</ul>
<hr>
<p>真正的给予使另一人也成为给予者，所以爱是一种生产爱的能力。</p>
<blockquote>
<p>马克思《1844年经济学-哲学手稿》：<br>
假定人就是人，而人跟世界的关系是一种合乎人本性的关系，那么，你就只能用爱来交换爱，只能用信任来交换信任，等等。<br>
如果你想得到艺术的享受，你本身必须是一个有艺术修养的人；如果你想感化别人，你本身就必须是一个能实际上鼓舞和推动别人前进的人。你跟人和自然的一切关系，都必须是同你意志的对象相符合的、你的现实个人生活的明确表现。<br>
如果你的爱作为爱没有引起对方对你的爱，如果你作为爱者用自己的生命表现没有成为被爱者，那么你的爱就是无力的，而这种爱就是不幸。</p>
</blockquote>
<hr>
<p>作为给予行为表现出来的爱的能力依靠人的性格发展，以获得某种以生产性为主的取向为前提。</p>
<ul>
<li>克服依赖、自恋的无限权力和利用他人的愿望或积存的愿望。</li>
<li>获得对自身能力的信心，获得依靠自己的能力去实现目标的勇气。</li>
</ul>
<hr>
<h3 id="爱的四要素">爱的四要素</h3>
<hr>
<p>关心</p>
<ul>
<li>爱是对我们所爱的生命和人或物成长的主动关注。</li>
</ul>
<hr>
<p>责任</p>
<ul>
<li>责任并不是义务，不是外部强加于人的。责任是完全自愿的行动，是我对另一个人精神需要的反应。</li>
</ul>
<hr>
<p>尊重</p>
<ul>
<li>尊重意味着</li>
</ul>
<h2 id="22-父母与孩子之间的爱">2.2 父母与孩子之间的爱</h2>
<h2 id="23-爱的对象">2.3 爱的对象</h2>
<ul>
<li>婴儿的分离感被母亲的在场所弥补。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[4.朴素贝叶斯法]]></title>
        <id>https://chensi01.github.io/post/4po-su-bei-xie-si-fa/</id>
        <link href="https://chensi01.github.io/post/4po-su-bei-xie-si-fa/">
        </link>
        <updated>2021-03-28T06:11:07.000Z</updated>
        <content type="html"><![CDATA[<h2 id="习题">习题</h2>
<h3 id="习题41">习题4.1</h3>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[3.k近邻法]]></title>
        <id>https://chensi01.github.io/post/3k-jin-lin-fa/</id>
        <link href="https://chensi01.github.io/post/3k-jin-lin-fa/">
        </link>
        <updated>2021-03-26T05:10:03.000Z</updated>
        <content type="html"><![CDATA[<h2 id="本章重点">本章重点</h2>
<h2 id="习题">习题</h2>
<p><a href="https://github.com/chensi01/lihang-solution-code">习题代码链接</a></p>
<h3 id="习题31">习题3.1</h3>
<figure data-type="image" tabindex="1"><img src="https://chensi01.github.io//post-images/1616735492285.png" alt="" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[2.感知机]]></title>
        <id>https://chensi01.github.io/post/2gan-zhi-ji/</id>
        <link href="https://chensi01.github.io/post/2gan-zhi-ji/">
        </link>
        <updated>2021-03-25T06:25:02.000Z</updated>
        <content type="html"><![CDATA[<h2 id="1本章重点">1.本章重点</h2>
<ul>
<li>感知机的模型、策略和算法。</li>
<li>感知机学习算法的收敛性和对偶形式。</li>
</ul>
<h2 id="2习题">2.习题</h2>
<h3 id="习题21">习题2.1</h3>
<p><strong>感知机模型是线性分类模型</strong>，对应于特征空间中将实例划分为正负两例的分离超平面。由下面的XOR的示意图可得，不存在一条直线将正类和负类分隔开，即<strong>XOR问题是线性不可分的</strong>。因此感知机不能表示异或。<br>
<img src="https://chensi01.github.io//post-images/1616659407689.png" alt="" loading="lazy"></p>
<pre><code class="language-python">#根据输入空间X和XOR规则生成实例
import pandas as pd
X,data=[-1,1],[]
for x1 in X:
    for x2 in X:
        y = -1 if (x1==x2) else 1
        data.append([x1,x2,y])
data = pd.DataFrame(data,index=None,columns=['x1','x2','y'])
print(data)
</code></pre>
<pre><code>   x1  x2  y
0  -1  -1 -1
1  -1   1  1
2   1  -1  1
3   1   1 -1
</code></pre>
<pre><code class="language-python">#根据XOR的数据实例绘制示意图
import matplotlib
from matplotlib import pyplot as plt 
plt.xlim(-2,2)
plt.ylim(-2,2)
plt.xticks(range(-2,3))
plt.yticks(range(-2,3))
plt.xlabel('x1')
plt.ylabel('x2')
plt.plot(data[data['y']==1]['x1'],data[data['y']==1]['x2'],'ro')
plt.plot(data[data['y']==-1]['x1'],data[data['y']==-1]['x2'],'gx')
plt.show()
</code></pre>
<h3 id="习题22">习题2.2</h3>
<pre><code class="language-python">import numpy as np
import pandas as pd
X_train = np.array([[3,3],[4,3],[1,1]])
Y = np.array([1,1,-1])
</code></pre>
<pre><code class="language-python">import matplotlib.pyplot as plt
def plot(w,b):
    # 绘制感知机
    X1 = [0,5]
    X2 = [-(b+w[0]*x1)/(w[1]+1e-7) for x1 in X1]
    plt.plot(X1,X2)
    #     
    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.xlim(0,5)
    plt.ylim(-3,5)
    plt.xticks(range(6))
    plt.yticks(range(-3,6))
    # 绘制训练数据
    plt.plot(X_train[Y==1][:,0],X_train[Y==1][:,1],'ro')
    plt.plot(X_train[Y==-1][:,0],X_train[Y==-1][:,1],'go')
    plt.show()
</code></pre>
<pre><code class="language-python">class Perceptron:
    def __init__(self):
        self.max_iter = 100
        self.lr = 1
        self.input_dim = 2
        self.build_model()
    def build_model(self):
        self.w = np.zeros(self.input_dim)
        self.b = 0
    def predict(self,x):
        output = np.matmul(self.w,x)+self.b
        return np.sign(output)
    def fit(self,X_train,Y):
        cur_iter = 0
        while cur_iter&lt;self.max_iter:
            fail_count = 0
            for x,y in zip(X_train,Y):
                y_hat=self.predict(x)
                if y*(np.matmul(self.w,x)+self.b)&lt;=0:
                    fail_count+=1
                    self.w += self.lr*y*x
                    self.b += self.lr*y
                    plot(self.w,self.b)
            if fail_count==0:
                break 
</code></pre>
<pre><code class="language-python">model = Perceptron()
model.fit(X_train,Y)
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://chensi01.github.io//post-images/1616670259364.png" alt="" loading="lazy"></figure>
<h3 id="习题23">习题2.3</h3>
<ul>
<li>凸壳可以看作是点集合的边界，可以用集合内所有点的线性组合构造。在二维的欧氏空间中，凸壳可以想做一条刚好包围这所有点的橡皮圈。</li>
<li><a href="https://datawhalechina.github.io/statistical-learning-method-solutions-manual/#/chapter2/chapter2?id=%e4%b9%a0%e9%a2%9822">习题解答</a></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[1.统计学习方法概论]]></title>
        <id>https://chensi01.github.io/post/1tong-ji-xue-xi-fang-fa-gai-lun/</id>
        <link href="https://chensi01.github.io/post/1tong-ji-xue-xi-fang-fa-gai-lun/">
        </link>
        <updated>2021-03-24T02:25:19.000Z</updated>
        <content type="html"><![CDATA[<h1 id="1本章内容">1.本章内容</h1>
<h2 id="基本概念">基本概念</h2>
<ul>
<li>统计学习是关于计算机基于数据构建概率统计模型，并运用模型对数据进行预测与分析的学科。</li>
<li>统计学习关于数据的基本假设：同类数据具有一定的统计规律性。</li>
<li>监督学习关于数据的基本假设：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span></span></span></span>具有联合概率分布<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo separator="true">,</mo><mi>Y</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(X,Y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span></span></span></span>，且训练数据和测试数据是依<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo separator="true">,</mo><mi>Y</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(X,Y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span></span></span></span>独立同分布产生的。</li>
<li>监督学习关于模型的假设：假设模型属于某个假设空间。</li>
<li>在监督学习中，输入和输出对称为样本；在无监督学习中输入是样本。</li>
</ul>
<h2 id="统计学习三要素模型-策略和算法">统计学习三要素：模型、策略和算法</h2>
<ul>
<li>损失函数是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">f(X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span></span></span></span>的非负实值函数。</li>
<li>损失函数度量模型一次预测的好坏，风险函数（期望损失）度量平均意义下模型预测的好坏。</li>
<li>期望风险：指损失函数的期望，代表理论上模型关于联合概率分布<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo separator="true">,</mo><mi>Y</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(X,Y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span></span></span></span>的平均意义下的损失。学习的目标是选择期望风险最小的模型，需要用到联合分布，但联合分布未知，所以监督学习是一个病态问题。</li>
<li>经验风险：模型关于训练数据集的平均损失。</li>
<li>结构风险：在经验风险上加上表示模型复杂度的正则化项。</li>
</ul>
<h2 id="正则化与交叉验证">正则化与交叉验证</h2>
<ul>
<li>从贝叶斯的角度看，正则化项对应模型的先验概率。可以假设复杂的模型有较小的先验概率，简单的模型有较大的先验概率。</li>
</ul>
<h2 id="泛化能力">泛化能力</h2>
<ul>
<li>模型对未知数据预测的误差即为泛化误差，即模型的期望风险。</li>
<li>泛化误差上界：通过比较学习方法的泛化误差上界来比较优劣。</li>
<li>泛化误差上界的性质：（1）样本容量的函数；（2）假设空间大小的函数。</li>
</ul>
<h2 id="生成模型与判别模型">生成模型与判别模型</h2>
<ul>
<li>监督学习方法分为生成方法和判别方法。（1）生成方法学习联合概率分布<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo separator="true">,</mo><mi>Y</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(X,Y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span></span></span></span>，然后求出条件概率分布<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(Y|X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span>；（2）判别方法直接学习条件概率分布<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(Y|X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span>或决策函数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">f(X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span>。</li>
</ul>
<h1 id="2习题">2.习题</h1>
<ul>
<li>极大似然估计（Maximum Likelihood Estimation，MLE）和贝叶斯估计（Bayesian Estimation）是统计推断中两种最常用的参数估计方法</li>
<li><a href="https://datawhalechina.github.io/statistical-learning-method-solutions-manual/#/chapter1/chapter1">习题解答</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/61593112">极大似然估计和贝叶斯估计</a></li>
</ul>
]]></content>
    </entry>
</feed>