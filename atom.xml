<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://chensi01.github.io/</id>
    <title>Chensi&apos;s Blog</title>
    <updated>2021-03-28T11:43:56.029Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://chensi01.github.io/"/>
    <link rel="self" href="https://chensi01.github.io/atom.xml"/>
    <subtitle>Welcome</subtitle>
    <logo>https://chensi01.github.io/images/avatar.png</logo>
    <icon>https://chensi01.github.io/favicon.ico</icon>
    <rights>All rights reserved 2021, Chensi&apos;s Blog</rights>
    <entry>
        <title type="html"><![CDATA[《爱的艺术》]]></title>
        <id>https://chensi01.github.io/post/lesslessai-de-yi-zhu-greatergreater/</id>
        <link href="https://chensi01.github.io/post/lesslessai-de-yi-zhu-greatergreater/">
        </link>
        <updated>2021-03-28T10:53:58.000Z</updated>
        <content type="html"><![CDATA[<h2 id="序">序</h2>
<ul>
<li>爱，并不是任何人都可以享受的生活情趣，而是与人的成熟程度相关联的。如果一个人不是及其主动地发展自己的全部人格，使自己具有一种能动的生活态度，那么对爱付出的努力必将如同付诸流水。</li>
<li>爱是一门艺术，需要知识和努力。</li>
</ul>
<h2 id="第一章-爱是一门艺术吗">第一章 爱是一门艺术吗？</h2>
<p>为什么大家觉得爱不需要学习：</p>
<ul>
<li>把爱的问题看作“被爱”的问题，而不是主动去爱和爱的能力的问题。爱的问题是如何能够被爱，如何变得可爱。</li>
<li>把爱的问题看作关于“对象”的问题，而不是身心能力的问题。为什么看中“对象”：整个文化以后买鱼和互利交换的观念为基础。只有当对方“人性商品”和自己可能交换时，坠入情网的感觉才会发展起来。我尽力满足成交条件，对象也应从社会价值的立场认为我称其心意。两个人考虑各自的交换价值的限度，认为对方是市场上能得到的最佳对象。</li>
<li>把“堕入”爱网时的最初体验和存在于爱之中的持久状态混淆。fall in love v.s. be in love。前者可能只证明了先前的孤独程度。</li>
</ul>
<p>爱是一门艺术，正如生活是一门艺术；想学会如何爱，就必须想学习音乐、绘画等艺术一样着手。学习艺术的过程分为精通理论和善于实践。要成为大师，还需要必须认为这门艺术是最高的旨趣所在。而尽管人们对爱的渴望根深蒂固，但我们总是把成功、威望、金钱、权力看得比爱重要并投入大量精力。</p>
<h2 id="第二章-爱的理论">第二章 爱的理论</h2>
<h3 id="21-爱-对人类存在问题的解答">2.1 爱-对人类存在问题的解答</h3>
<p>人生而面临不确定的状态，唯有过去是确定的，关于未来则只有死亡是确定的。人意识到自己是一个分离的实体，这成了严重焦虑的根源。人类最深切的需要，就是克服这种分离，脱离其只身一人的牢笼。</p>
<p>常见的克服分离的途径只是片面的答案：</p>
<ul>
<li>狂欢状态：性高潮、酗酒、毒瘾。狂欢的体验结束之后，会感到更加分离，于是更加求助于狂欢。【短暂的联合】</li>
<li>与群体融合：个人的自我消失，归属于群体的自我。【虚假的联合】</li>
<li>创造性活动：自身的材料融于一体。【不是人与人之间的联合】<br>
爱能实现全面的人与人之间的融合。</li>
</ul>
<p>爱的不成熟形式：“共生性融合”。其消极和积极形式分别是受虐狂和施虐狂。</p>
<ul>
<li>
<p>受虐狂：他是一切，我只是他的一部分，我是崇高、权力以及确定的一部分。不必做确定和冒风险。</p>
</li>
<li>
<p>施虐狂：通过别人的崇拜来抬高和强化自己。<br>
这是不完整的联合，受虐和施虐的态度往往在一个人身上用于对待不同对象。希特勒以施虐狂的方式对待人民，以受虐狂的方式对待命运、历史和自然的更高力量。</p>
</li>
<li>
<p>婴儿的分离感被母亲的在场所弥补。</p>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[4.朴素贝叶斯法]]></title>
        <id>https://chensi01.github.io/post/4po-su-bei-xie-si-fa/</id>
        <link href="https://chensi01.github.io/post/4po-su-bei-xie-si-fa/">
        </link>
        <updated>2021-03-28T06:11:07.000Z</updated>
        <content type="html"><![CDATA[<h2 id="习题">习题</h2>
<h3 id="习题41">习题4.1</h3>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[3.k近邻法]]></title>
        <id>https://chensi01.github.io/post/3k-jin-lin-fa/</id>
        <link href="https://chensi01.github.io/post/3k-jin-lin-fa/">
        </link>
        <updated>2021-03-26T05:10:03.000Z</updated>
        <content type="html"><![CDATA[<h2 id="本章重点">本章重点</h2>
<h2 id="习题">习题</h2>
<p><a href="https://github.com/chensi01/lihang-solution-code">习题代码链接</a></p>
<h3 id="习题31">习题3.1</h3>
<figure data-type="image" tabindex="1"><img src="https://chensi01.github.io//post-images/1616735492285.png" alt="" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[2.感知机]]></title>
        <id>https://chensi01.github.io/post/2gan-zhi-ji/</id>
        <link href="https://chensi01.github.io/post/2gan-zhi-ji/">
        </link>
        <updated>2021-03-25T06:25:02.000Z</updated>
        <content type="html"><![CDATA[<h2 id="1本章重点">1.本章重点</h2>
<ul>
<li>感知机的模型、策略和算法。</li>
<li>感知机学习算法的收敛性和对偶形式。</li>
</ul>
<h2 id="2习题">2.习题</h2>
<h3 id="习题21">习题2.1</h3>
<p><strong>感知机模型是线性分类模型</strong>，对应于特征空间中将实例划分为正负两例的分离超平面。由下面的XOR的示意图可得，不存在一条直线将正类和负类分隔开，即<strong>XOR问题是线性不可分的</strong>。因此感知机不能表示异或。<br>
<img src="https://chensi01.github.io//post-images/1616659407689.png" alt="" loading="lazy"></p>
<pre><code class="language-python">#根据输入空间X和XOR规则生成实例
import pandas as pd
X,data=[-1,1],[]
for x1 in X:
    for x2 in X:
        y = -1 if (x1==x2) else 1
        data.append([x1,x2,y])
data = pd.DataFrame(data,index=None,columns=['x1','x2','y'])
print(data)
</code></pre>
<pre><code>   x1  x2  y
0  -1  -1 -1
1  -1   1  1
2   1  -1  1
3   1   1 -1
</code></pre>
<pre><code class="language-python">#根据XOR的数据实例绘制示意图
import matplotlib
from matplotlib import pyplot as plt 
plt.xlim(-2,2)
plt.ylim(-2,2)
plt.xticks(range(-2,3))
plt.yticks(range(-2,3))
plt.xlabel('x1')
plt.ylabel('x2')
plt.plot(data[data['y']==1]['x1'],data[data['y']==1]['x2'],'ro')
plt.plot(data[data['y']==-1]['x1'],data[data['y']==-1]['x2'],'gx')
plt.show()
</code></pre>
<h3 id="习题22">习题2.2</h3>
<pre><code class="language-python">import numpy as np
import pandas as pd
X_train = np.array([[3,3],[4,3],[1,1]])
Y = np.array([1,1,-1])
</code></pre>
<pre><code class="language-python">import matplotlib.pyplot as plt
def plot(w,b):
    # 绘制感知机
    X1 = [0,5]
    X2 = [-(b+w[0]*x1)/(w[1]+1e-7) for x1 in X1]
    plt.plot(X1,X2)
    #     
    plt.xlabel('x1')
    plt.ylabel('x2')
    plt.xlim(0,5)
    plt.ylim(-3,5)
    plt.xticks(range(6))
    plt.yticks(range(-3,6))
    # 绘制训练数据
    plt.plot(X_train[Y==1][:,0],X_train[Y==1][:,1],'ro')
    plt.plot(X_train[Y==-1][:,0],X_train[Y==-1][:,1],'go')
    plt.show()
</code></pre>
<pre><code class="language-python">class Perceptron:
    def __init__(self):
        self.max_iter = 100
        self.lr = 1
        self.input_dim = 2
        self.build_model()
    def build_model(self):
        self.w = np.zeros(self.input_dim)
        self.b = 0
    def predict(self,x):
        output = np.matmul(self.w,x)+self.b
        return np.sign(output)
    def fit(self,X_train,Y):
        cur_iter = 0
        while cur_iter&lt;self.max_iter:
            fail_count = 0
            for x,y in zip(X_train,Y):
                y_hat=self.predict(x)
                if y*(np.matmul(self.w,x)+self.b)&lt;=0:
                    fail_count+=1
                    self.w += self.lr*y*x
                    self.b += self.lr*y
                    plot(self.w,self.b)
            if fail_count==0:
                break 
</code></pre>
<pre><code class="language-python">model = Perceptron()
model.fit(X_train,Y)
</code></pre>
<figure data-type="image" tabindex="1"><img src="https://chensi01.github.io//post-images/1616670259364.png" alt="" loading="lazy"></figure>
<h3 id="习题23">习题2.3</h3>
<ul>
<li>凸壳可以看作是点集合的边界，可以用集合内所有点的线性组合构造。在二维的欧氏空间中，凸壳可以想做一条刚好包围这所有点的橡皮圈。</li>
<li><a href="https://datawhalechina.github.io/statistical-learning-method-solutions-manual/#/chapter2/chapter2?id=%e4%b9%a0%e9%a2%9822">习题解答</a></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[1.统计学习方法概论]]></title>
        <id>https://chensi01.github.io/post/1tong-ji-xue-xi-fang-fa-gai-lun/</id>
        <link href="https://chensi01.github.io/post/1tong-ji-xue-xi-fang-fa-gai-lun/">
        </link>
        <updated>2021-03-24T02:25:19.000Z</updated>
        <content type="html"><![CDATA[<h1 id="1本章内容">1.本章内容</h1>
<h2 id="基本概念">基本概念</h2>
<ul>
<li>统计学习是关于计算机基于数据构建概率统计模型，并运用模型对数据进行预测与分析的学科。</li>
<li>统计学习关于数据的基本假设：同类数据具有一定的统计规律性。</li>
<li>监督学习关于数据的基本假设：<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span></span></span></span>具有联合概率分布<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo separator="true">,</mo><mi>Y</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(X,Y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span></span></span></span>，且训练数据和测试数据是依<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo separator="true">,</mo><mi>Y</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(X,Y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span></span></span></span>独立同分布产生的。</li>
<li>监督学习关于模型的假设：假设模型属于某个假设空间。</li>
<li>在监督学习中，输入和输出对称为样本；在无监督学习中输入是样本。</li>
</ul>
<h2 id="统计学习三要素模型-策略和算法">统计学习三要素：模型、策略和算法</h2>
<ul>
<li>损失函数是<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">f(X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span>和<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>Y</mi></mrow><annotation encoding="application/x-tex">Y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span></span></span></span>的非负实值函数。</li>
<li>损失函数度量模型一次预测的好坏，风险函数（期望损失）度量平均意义下模型预测的好坏。</li>
<li>期望风险：指损失函数的期望，代表理论上模型关于联合概率分布<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo separator="true">,</mo><mi>Y</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(X,Y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span></span></span></span>的平均意义下的损失。学习的目标是选择期望风险最小的模型，需要用到联合分布，但联合分布未知，所以监督学习是一个病态问题。</li>
<li>经验风险：模型关于训练数据集的平均损失。</li>
<li>结构风险：在经验风险上加上表示模型复杂度的正则化项。</li>
</ul>
<h2 id="正则化与交叉验证">正则化与交叉验证</h2>
<ul>
<li>从贝叶斯的角度看，正则化项对应模型的先验概率。可以假设复杂的模型有较小的先验概率，简单的模型有较大的先验概率。</li>
</ul>
<h2 id="泛化能力">泛化能力</h2>
<ul>
<li>模型对未知数据预测的误差即为泛化误差，即模型的期望风险。</li>
<li>泛化误差上界：通过比较学习方法的泛化误差上界来比较优劣。</li>
<li>泛化误差上界的性质：（1）样本容量的函数；（2）假设空间大小的函数。</li>
</ul>
<h2 id="生成模型与判别模型">生成模型与判别模型</h2>
<ul>
<li>监督学习方法分为生成方法和判别方法。（1）生成方法学习联合概率分布<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>X</mi><mo separator="true">,</mo><mi>Y</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(X,Y)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mclose">)</span></span></span></span>，然后求出条件概率分布<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(Y|X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span>；（2）判别方法直接学习条件概率分布<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>P</mi><mo>(</mo><mi>Y</mi><mi mathvariant="normal">∣</mi><mi>X</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">P(Y|X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.22222em;">Y</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span>或决策函数<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>f</mi><mo>(</mo><mi>X</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">f(X)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.07847em;">X</span><span class="mclose">)</span></span></span></span>。</li>
</ul>
<h1 id="2习题">2.习题</h1>
<ul>
<li>极大似然估计（Maximum Likelihood Estimation，MLE）和贝叶斯估计（Bayesian Estimation）是统计推断中两种最常用的参数估计方法</li>
<li><a href="https://datawhalechina.github.io/statistical-learning-method-solutions-manual/#/chapter1/chapter1">习题解答</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/61593112">极大似然估计和贝叶斯估计</a></li>
</ul>
]]></content>
    </entry>
</feed>